{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3c89ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped existing SparkSession.\n",
      "✅ PySpark internals reset\n",
      "✅ Spark started: 3.5.8\n",
      " - Master: local[*]\n",
      " - Spark UI: http://192.168.55.108:4040\n",
      "alive_check = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- 1) Stop any existing SparkSession (if any) ----------\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped existing SparkSession.\")\n",
    "except Exception:\n",
    "    print(\"No SparkSession to stop (ok).\")\n",
    "\n",
    "# ---------- 2) Clear PySpark global state (fixes dead Py4J gateway) ----------\n",
    "try:\n",
    "    from pyspark.context import SparkContext\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    for k in [\"PYSPARK_GATEWAY_PORT\", \"PYSPARK_GATEWAY_SECRET\"]:\n",
    "        if k in os.environ:\n",
    "            os.environ.pop(k, None)\n",
    "            print(f\"Cleared env {k}\")\n",
    "\n",
    "    SparkContext._active_spark_context = None\n",
    "    SparkContext._gateway = None\n",
    "    SparkContext._jvm = None\n",
    "\n",
    "    try:\n",
    "        SparkSession._instantiatedContext = None\n",
    "        SparkSession._activeSession = None\n",
    "        SparkSession._defaultSession = None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"✅ PySpark internals reset\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Reset step warning:\", type(e).__name__, e)\n",
    "\n",
    "# ---------- 3) Start Spark (stable local mode) ----------\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "OUTPUT_ROOT = Path.home() / \"opendota_processed\"\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SPARK_LOCAL_DIR = OUTPUT_ROOT / \"spark_local\"\n",
    "SPARK_LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"OpenDota_Notebook\")\n",
    "    .master(\"local[*]\")  # stable; we can change later\n",
    "    .config(\"spark.local.dir\", str(SPARK_LOCAL_DIR))\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"400\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    # IMPORTANT: avoid surprise broadcasts that can destabilize the JVM\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"✅ Spark started:\", spark.version)\n",
    "print(\" - Master:\", spark.sparkContext.master)\n",
    "print(\" - Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"alive_check =\", spark.range(1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b173568d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped SparkSession.\n",
      "✅ Spark started: 3.5.8\n",
      " - Master: local[4]\n",
      " - Spark UI: http://192.168.55.108:4040\n",
      "alive_check = 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Stop if possible\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Stopped SparkSession.\")\n",
    "except Exception:\n",
    "    print(\"No SparkSession to stop (ok).\")\n",
    "\n",
    "# 2) Hard reset PySpark globals (dead gateway fix)\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "for k in [\"PYSPARK_GATEWAY_PORT\", \"PYSPARK_GATEWAY_SECRET\"]:\n",
    "    os.environ.pop(k, None)\n",
    "\n",
    "SparkContext._active_spark_context = None\n",
    "SparkContext._gateway = None\n",
    "SparkContext._jvm = None\n",
    "SparkSession._instantiatedContext = None\n",
    "SparkSession._activeSession = None\n",
    "SparkSession._defaultSession = None\n",
    "\n",
    "# 3) Start Spark (stable settings)\n",
    "OUTPUT_ROOT = Path.home() / \"opendota_processed\"\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "SPARK_LOCAL_DIR = OUTPUT_ROOT / \"spark_local\"\n",
    "SPARK_LOCAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"OpenDota_Preprocessing_Notebook_Stable\")\n",
    "    .master(\"local[4]\")                 # <-- reduce threads for stability\n",
    "    .config(\"spark.ui.enabled\", \"true\")# <-- remove UI port issues while debugging\n",
    "    .config(\"spark.local.dir\", str(SPARK_LOCAL_DIR))\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"32\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")  # <-- disable AQE for now\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"✅ Spark started:\", spark.version)\n",
    "print(\" - Master:\", spark.sparkContext.master)\n",
    "print(\" - Spark UI:\", spark.sparkContext.uiWebUrl)\n",
    "print(\"alive_check =\", spark.range(1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9002e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data_output/part-*.csv\" #ibahin path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "188f05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, IntegerType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"match_id\", LongType(), True),\n",
    "    StructField(\"match_seq_num\", LongType(), True), # Needed to maintain index position\n",
    "    StructField(\"radiant_win\", StringType(), True),\n",
    "    StructField(\"start_time\", LongType(), True),\n",
    "    StructField(\"duration\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_raw = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(input_path)\n",
    "\n",
    "df_matches = df_raw.select(\"match_id\", \"radiant_win\", \"start_time\", \"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dab763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+--------+\n",
      "| match_id|radiant_win|start_time|duration|\n",
      "+---------+-----------+----------+--------+\n",
      "|473260719|          t|1390019262|    3234|\n",
      "|473260722|          f|1390019274|    2360|\n",
      "|473260723|          f|1390019260|    2546|\n",
      "|473260724|          f|1390019278|    2647|\n",
      "|473260725|          t|1390019250|    3030|\n",
      "+---------+-----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_matches.select(\"match_id\",\"radiant_win\", \"start_time\", \"duration\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70802aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# count of matches before drop\n",
    "before_count = df_matches.count()\n",
    "\n",
    "# constructing clean match data\n",
    "matches_clean = df_matches.withColumn(\"match_id\", F.col(\"match_id\").cast(\"long\")) \\\n",
    "                          .withColumn(\"start_time\", F.col(\"start_time\").cast(\"long\")) \\\n",
    "                          .withColumn(\"duration\", F.col(\"duration\").cast(\"int\")) \\\n",
    "                          .withColumn(\"radiant_win\", F.col(\"radiant_win\") == \"t\") \\\n",
    "                          .dropna(subset=[\"match_id\"]) \\\n",
    "                          .dropDuplicates([\"match_id\"])\n",
    "\n",
    "# Justification: Matches < 5 mins (300s) are usually 'null' games (early abandons).\n",
    "# Matches > 3 hours (10800s) are extreme outliers/server errors in Dota 2.\n",
    "lower_bound = 300\n",
    "upper_bound = 10800\n",
    "matches_clean = matches_clean.filter((F.col(\"duration\") >= lower_bound) & (F.col(\"duration\") <= upper_bound))\n",
    "\n",
    "# D) Capture After Count\n",
    "after_count = matches_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1593c375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TASK 4: EVIDENCE ---\n",
      "Before cleaning: 500000 rows\n",
      "After cleaning:  497320 rows\n",
      "Rows removed:    2680\n",
      "\n",
      "--- Null Rate Summary (%) ---\n",
      "+--------+--------------------+----------+--------+\n",
      "|match_id|         radiant_win|start_time|duration|\n",
      "+--------+--------------------+----------+--------+\n",
      "|     0.0|2.010777768840987...|       0.0|     0.0|\n",
      "+--------+--------------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# null rate calc\n",
    "null_rate_df = matches_clean.select([\n",
    "    (F.count(F.when(F.col(c).isNull(), c)) / after_count * 100).alias(c) \n",
    "    for c in matches_clean.columns\n",
    "])\n",
    "\n",
    "print(\"--- TASK 4: EVIDENCE ---\")\n",
    "print(f\"Before cleaning: {before_count} rows\")\n",
    "print(f\"After cleaning:  {after_count} rows\")\n",
    "print(f\"Rows removed:    {before_count - after_count}\")\n",
    "\n",
    "print(\"\\n--- Null Rate Summary (%) ---\")\n",
    "null_rate_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209401a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- match_id: long (nullable = true)\n",
      " |-- radiant_win: boolean (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      "\n",
      "+----------+-----------+----------+--------+\n",
      "|  match_id|radiant_win|start_time|duration|\n",
      "+----------+-----------+----------+--------+\n",
      "| 473260800|      false|1390019708|    1843|\n",
      "|1009842190|      false|1415362477|    2950|\n",
      "|1808667856|      false|1442797852|    3096|\n",
      "| 692810562|      false|1401589879|    2141|\n",
      "|1980325600|       true|1449295127|    2140|\n",
      "+----------+-----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matches_clean.printSchema()\n",
    "matches_clean.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23c2e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----+-----+---+------------------+------------+\n",
      "|  match_id|           start_ts|year|month|day|      duration_min|duration_bin|\n",
      "+----------+-------------------+----+-----+---+------------------+------------+\n",
      "| 473260800|2014-01-18 12:35:08|2014|    1| 18|30.716666666666665|       30-40|\n",
      "|1009842190|2014-11-07 20:14:37|2014|   11|  7|49.166666666666664|       40-50|\n",
      "|1808667856|2015-09-21 09:10:52|2015|    9| 21|              51.6|       50-60|\n",
      "| 692810562|2014-06-01 10:31:19|2014|    6|  1| 35.68333333333333|       30-40|\n",
      "|1980325600|2015-12-05 13:58:47|2015|   12|  5|35.666666666666664|       30-40|\n",
      "+----------+-------------------+----+-----+---+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# derive day, year, month, and duration_min\n",
    "matches_features = matches_clean.withColumn(\"start_ts\", F.from_unixtime(\"start_time\").cast(\"timestamp\")) \\\n",
    "    .withColumn(\"year\", F.year(\"start_ts\")) \\\n",
    "    .withColumn(\"month\", F.month(\"start_ts\")) \\\n",
    "    .withColumn(\"day\", F.day(\"start_ts\")) \\\n",
    "    .withColumn(\"duration_min\", F.col(\"duration\") / 60)\n",
    "\n",
    "# bins: <20, 20–30, 30-40, 40-50, 50-60, 60+\n",
    "matches_features = matches_features.withColumn(\"duration_bin\", \n",
    "    F.when(F.col(\"duration_min\") < 20, \"<20\")\n",
    "     .when((F.col(\"duration_min\") >= 20) & (F.col(\"duration_min\") < 30), \"20-30\")\n",
    "     .when((F.col(\"duration_min\") >= 30) & (F.col(\"duration_min\") < 40), \"30-40\")\n",
    "     .when((F.col(\"duration_min\") >= 40) & (F.col(\"duration_min\") < 50), \"40-50\")\n",
    "     .when((F.col(\"duration_min\") >= 50) & (F.col(\"duration_min\") < 60), \"50-60\")\n",
    "     .otherwise(\"60+\")\n",
    ")\n",
    "\n",
    "# preview of newest features \n",
    "matches_features.select(\"match_id\", \"start_ts\", \"year\", \"month\", \"day\", \"duration_min\", \"duration_bin\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d1954a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully written to: data/matches_features.parquet\n"
     ]
    }
   ],
   "source": [
    "# define the output path\n",
    "parquet_path = f\"data/matches_features.parquet\"\n",
    "\n",
    "# write the data partitioned by year and month\n",
    "matches_features.write.mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(parquet_path)\n",
    "\n",
    "print(f\"✅ Data successfully written to: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61037ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(f\"ls -R {parquet_path} | head -n 20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = spark.read.parquet(parquet_path)\n",
    "filtered_query = df_p.filter((F.col(\"year\") == 2014) & (F.col(\"month\") == 1))\n",
    "filtered_query.explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
